{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning\n",
    "\n",
    "Now that we have our data ready it's time to get a model.\n",
    "\n",
    "One of the current best places to find trained models for use is [Hugging Face](https://huggingface.co/). Not only is Hugging Face a great repository of Gen AI models, they've also developed a number of python libraries for working with models that we'll use here.\n",
    "\n",
    "We'll be attempting to do this all locally so we'll not need to create any accounts with Hugging Face.\n",
    "\n",
    "In addition to libraries provided by Hugging Face we'll use [LangChain](https://www.langchain.com/) to streamline the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "#import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from trl import SFTTrainer\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where to start\n",
    "---\n",
    "\n",
    "As part of this process we want to see how a model can improve after being fine tuned. So instead of choosing a model that has been specifically designed for code assistance we'll take a very baseline model and see what improvements we can get.\n",
    "\n",
    "For that we'll use a base gpt2 model with a smaller number of parameters.\n",
    "\n",
    "Many Gen AI models in production have parameter values in the billions which would be very time intensive to try to fine tune without access to dedicated GPUs so we'll see what hurdles we face on a smaller model being trained directly on the CPU\n",
    "\n",
    "The hugging face libraries allow us to download both the model and the associated tokenizer worked with it.\n",
    "\n",
    "In order to give the model an input and receive and output we'll create a model and pipeline doing the following:\n",
    "1. Use a model and tokenizer from Hugging Face\n",
    "2. Put everything in a pipeline\n",
    "3. Create a local model\n",
    "4. Ask a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a Hugging Face model and tokenizer\n",
    "\n",
    "model_link = \"openai-community/gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_link)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=100, # Adjust this for longer answers\n",
    "    pad_token_id=50256 # Setting the token to this value allows for open ended generation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local model\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Questions\n",
    "\n",
    "We'll ask some basic questions related to MongoDB to get an idea on how well the model currently knows the subject matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jacob.Ellena/miniconda3/envs/gen_ai_hackathon/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n'\n",
      " '\\n'\n",
      " 'Mongodb is a simple Python script for displaying JSON formatted results for '\n",
      " 'users of Google Analytics. It supports JSON formatting at compile, with a '\n",
      " 'few settings that help determine just how much data is displayed and not '\n",
      " 'what it is. It also sends \"message\" to the browser when a browser doesn\\'t '\n",
      " 'see any MONGODB.\\n'\n",
      " '\\n'\n",
      " \"To start listening for mongodb, simply open your own web browser and you'll \"\n",
      " 'see this message')\n"
     ]
    }
   ],
   "source": [
    "pprint(local_llm(\"What is mongodb?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' If you have an option enter the date using the following format: date = '\n",
      " 'monday, Monday, Tuesday, Wednesday night, Thursday, Friday Sunday, and '\n",
      " 'Saturday Monday, Tuesday, Thursday, Friday, Saturday, Sunday, and Monday day '\n",
      " 'or Sunday and Sunday.\\n'\n",
      " '\\n'\n",
      " \"You can convert to MIME Type if you don't have a date converter, or convert \"\n",
      " 'it to MIME type without your date converter.\\n'\n",
      " '\\n'\n",
      " 'Here is an example')\n"
     ]
    }
   ],
   "source": [
    "pprint(local_llm(\"How do I match by date in mongodb?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n'\n",
      " '\\n'\n",
      " '1. Install a custom function with your current user base! When launching '\n",
      " 'ng-app:\\n'\n",
      " '\\n'\n",
      " '1. Create a new mongo project.\\n'\n",
      " '\\n'\n",
      " '2. Place mongo project in your ng-app:\\n'\n",
      " '\\n'\n",
      " '2. Type this code to generate results in the following format;\\n'\n",
      " '\\n'\n",
      " '3. The output you see in the following output is your custom function. To '\n",
      " 'view the list of your custom functions')\n"
     ]
    }
   ],
   "source": [
    "pprint(local_llm(\"How to use a custom function in a mongodb aggregation pipeline?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answers\n",
    "\n",
    "Well as we can see the model currently doesn't know much about Mongodb.\n",
    "\n",
    "The next step is to try to fine tune the model on our local machine to see if we can get an improved output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning\n",
    "\n",
    "We'll use Hugging Face's training and dataset libraries to bring in our data and tune the model\n",
    "\n",
    "First we'll import our formatted CSV into a pandas dataframe then create a Hugging Face dataset type for use in our trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>### Question: Mongoose findById is not returni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>### Question: Why is my mongo collection being...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>### Question: MongoDb score results based on s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>### Question: Laravel 5.7 mongodb atlas connec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>### Question: Remote Mongo DB connection throu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87085</th>\n",
       "      <td>### Question: Validator error when POSTing. Cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87086</th>\n",
       "      <td>### Question: Does Meteor-JS support offline s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87087</th>\n",
       "      <td>### Question: Update a given mongo field in un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87088</th>\n",
       "      <td>### Question: MongoDB search - find newest wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87089</th>\n",
       "      <td>### Question: Scalable voting system with Mong...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87090 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0      ### Question: Mongoose findById is not returni...\n",
       "1      ### Question: Why is my mongo collection being...\n",
       "2      ### Question: MongoDb score results based on s...\n",
       "3      ### Question: Laravel 5.7 mongodb atlas connec...\n",
       "4      ### Question: Remote Mongo DB connection throu...\n",
       "...                                                  ...\n",
       "87085  ### Question: Validator error when POSTing. Cr...\n",
       "87086  ### Question: Does Meteor-JS support offline s...\n",
       "87087  ### Question: Update a given mongo field in un...\n",
       "87088  ### Question: MongoDB search - find newest wit...\n",
       "87089  ### Question: Scalable voting system with Mong...\n",
       "\n",
       "[87090 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.read_csv(\"./data/csv_template_formatted.csv\")\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 87090\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=Dataset.from_pandas(dataset_df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3471e012edbc4fc4a33af012b9452b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87090 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jacob.Ellena/miniconda3/envs/gen_ai_hackathon/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    #tokenizer=tokenizer,\n",
    "    max_seq_length= 512\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "The next step would be to run the following code:\n",
    "\n",
    "```python\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "I let the model train for about six days only was about 32% done.\n",
    "\n",
    "![Training Time](./images/training_time.png)\n",
    "\n",
    "So as we can see from the attached image that while the model is indeed training the time it would take is a bit longer then expected for easy model evaluation and experimentation\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "This isn't to say that we still can't proceed in training the model this way but for some exploratory learnings the time commitment here may be a bit much. So how do we try to resolve this?\n",
    "\n",
    "##### Parameter Tuning\n",
    "---\n",
    "The libraries provided by Hugging Face have a large number of parameters that can be adjusted. Specifically modifying the tokenization may help speed up the training process.\n",
    "\n",
    "##### GPUs\n",
    "---\n",
    "We ignore the earlier statement of having a model train on any machine and specifically use one with a compatible GPU. This would drastically cut down on the training time\n",
    "\n",
    "##### OS Specific Libraries\n",
    "---\n",
    "If you're using a Mac with an M(X) chip Apple has been developing libraries to allow machine learning to tap into the computer's GPU\n",
    "\n",
    "[Tensorflow Metal](https://pypi.org/project/tensorflow-metal/)\n",
    "\n",
    "[Hugging Face Apple GPU-Acceleration](https://huggingface.co/docs/accelerate/en/usage_guides/mps#how-it-works-out-of-the-box)\n",
    "\n",
    "##### Different Tuning Methods\n",
    "---\n",
    "There are variety ways to fine tune a model. We tried training on all the available parameters here and was we saw it was very resource intensive. One method we could try to attempt next would be [Low-Rank Adaption of Large Language Models](https://huggingface.co/docs/diffusers/en/training/lora) as this trains a smaller number of weights.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai_hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
