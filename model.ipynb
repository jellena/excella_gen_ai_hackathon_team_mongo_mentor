{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning\n",
    "\n",
    "Now that we have our data ready, it's time to get a model.\n",
    "\n",
    "One of the current best places to find trained models for use is [Hugging Face](https://huggingface.co/). Not only is Hugging Face a great repository of Gen AI models, they've also developed a number of python libraries for working with models that we'll use here.\n",
    "\n",
    "We'll be attempting to do this all locally so we don't need to create an account with Hugging Face.\n",
    "\n",
    "In addition to libraries provided by Hugging Face, we'll use [LangChain](https://www.langchain.com/) to streamline the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from trl import SFTTrainer\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where to start\n",
    "---\n",
    "\n",
    "As part of this process we want to see how a model can improve after being fine tuned. So instead of choosing a model that has been specifically designed for code assistance we'll take a very baseline model and see what improvements we can get.\n",
    "\n",
    "For that we'll use a base gpt2 model with a smaller number of parameters.\n",
    "\n",
    "Many Gen AI models in production have parameter values in the billions which would be very time intensive to try to fine tune without access to dedicated GPUs, so we'll see what hurdles we face on a smaller model being trained directly on the CPU.\n",
    "\n",
    "The hugging face libraries allow us to download both the model and the associated tokenizer worked with it.\n",
    "\n",
    "In order to give the model an input and receive an output we'll create a model and pipeline doing the following:\n",
    "1. Use a model and tokenizer from Hugging Face\n",
    "2. Put everything in a pipeline\n",
    "3. Create a local model\n",
    "4. Ask a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a Hugging Face model and tokenizer\n",
    "\n",
    "model_link = \"openai-community/gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_link)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=100, # Adjust this for longer answers\n",
    "    pad_token_id=50256 # Setting the token to this value allows for open ended generation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local model\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Questions\n",
    "\n",
    "We'll ask some basic questions related to MongoDB to get an idea on how well the model currently knows the subject matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jacob.Ellena/miniconda3/envs/gen_ai_hackathon/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n'\n",
      " '\\n'\n",
      " 'Mongodb is an advanced command-line tool. It works by adding text or numbers '\n",
      " 'to commands or other commands. We have been using it for a while on Windows '\n",
      " 'XP and will probably add nbmw after upgrading to Windows 7 or more. If '\n",
      " \"you're using a desktop computer, it will work. It should work, but it is \"\n",
      " 'slow.\\n'\n",
      " '\\n'\n",
      " \"To use mongodb you'll need to install a basic command line tool called\")\n"
     ]
    }
   ],
   "source": [
    "pprint(local_llm(\"What is mongodb?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('¶ To solve this problem, the first step is to generate a mongodb.json which '\n",
      " 'contains the key sequences for the given date and time. The result should '\n",
      " 'look something along the lines of:\\n'\n",
      " '\\n'\n",
      " 'Date: 2011-02-02 02:22:51 Date: 2013-01-25 16:34:53 Date: 2010-05-13 '\n",
      " '12:50:17\\n'\n",
      " '\\n'\n",
      " 'You can go ahead and generate it')\n"
     ]
    }
   ],
   "source": [
    "pprint(local_llm(\"How do I match by date in mongodb?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n'\n",
      " '\\n'\n",
      " '$mongodb = new MyMongoDb(); $result = $mongodb->query(function($query) { '\n",
      " 'return $query->isAllowed(true); }); $result->query(function($q, $o){ return '\n",
      " '$q; }); $result->function(function($t){ return $t; }); '\n",
      " '$result->query(function($g){ return $g;')\n"
     ]
    }
   ],
   "source": [
    "pprint(local_llm(\"How to use a custom function in a mongodb aggregation pipeline?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answers\n",
    "\n",
    "Well, as we can see, the model currently doesn't know much about Mongodb.\n",
    "\n",
    "The next step is to try to fine tune the model on our local machine to see if we can get an improved output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning\n",
    "\n",
    "We'll use Hugging Face's training and dataset libraries to bring in our data and tune the model.\n",
    "\n",
    "First we'll import our formatted CSV into a pandas dataframe then create a Hugging Face dataset type for use in our trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>### Question: Mongoose findById is not returni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>### Question: Why is my mongo collection being...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>### Question: MongoDb score results based on s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>### Question: Laravel 5.7 mongodb atlas connec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>### Question: Remote Mongo DB connection throu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87085</th>\n",
       "      <td>### Question: Validator error when POSTing. Cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87086</th>\n",
       "      <td>### Question: Does Meteor-JS support offline s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87087</th>\n",
       "      <td>### Question: Update a given mongo field in un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87088</th>\n",
       "      <td>### Question: MongoDB search - find newest wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87089</th>\n",
       "      <td>### Question: Scalable voting system with Mong...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87090 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0      ### Question: Mongoose findById is not returni...\n",
       "1      ### Question: Why is my mongo collection being...\n",
       "2      ### Question: MongoDb score results based on s...\n",
       "3      ### Question: Laravel 5.7 mongodb atlas connec...\n",
       "4      ### Question: Remote Mongo DB connection throu...\n",
       "...                                                  ...\n",
       "87085  ### Question: Validator error when POSTing. Cr...\n",
       "87086  ### Question: Does Meteor-JS support offline s...\n",
       "87087  ### Question: Update a given mongo field in un...\n",
       "87088  ### Question: MongoDB search - find newest wit...\n",
       "87089  ### Question: Scalable voting system with Mong...\n",
       "\n",
       "[87090 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.read_csv(\"./data/csv_template_formatted\") # No file extension listed on purpose\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 87090\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=Dataset.from_pandas(dataset_df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47bf5a0a3b8941e89c6ce45986059989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87090 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jacob.Ellena/miniconda3/envs/gen_ai_hackathon/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length= 512\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "The next step would be to run the following code:\n",
    "\n",
    "```python\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "I let the model train for about six days, and it was only about 32% done.\n",
    "\n",
    "![Training Time](./images/training_time.png)\n",
    "\n",
    "While there isn't a clear conclusion yet on the efficacy of locally trained platform-agnostic model given resources and time constraints here are potential next steps to explore.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "This isn't to say that we still can't proceed in training the model this way, but for some exploratory learnings the time commitment here may be a bit much. So how do we try to resolve this?\n",
    "\n",
    "##### Parameter Tuning\n",
    "---\n",
    "The libraries provided by Hugging Face have a large number of parameters that can be adjusted. Specifically, modifying the tokenization may help speed up the training process.\n",
    "\n",
    "##### GPUs\n",
    "---\n",
    "We ignore the earlier statement of having a model train on any machine and specifically use one with a compatible GPU. This would drastically cut down on the training time.\n",
    "\n",
    "##### OS Specific Libraries\n",
    "---\n",
    "If you're using a Mac with an M(X) chip Apple has been developing libraries to allow machine learning to tap into the computer's GPU.\n",
    "\n",
    "[Tensorflow Metal](https://pypi.org/project/tensorflow-metal/)\n",
    "\n",
    "[Hugging Face Apple GPU-Acceleration](https://huggingface.co/docs/accelerate/en/usage_guides/mps#how-it-works-out-of-the-box)\n",
    "\n",
    "##### Different Tuning Methods\n",
    "---\n",
    "There are variety ways to fine tune a model. We tried training on all the available parameters here and saw that it was very resource intensive. One method we could try to attempt next would be [Low-Rank Adaption of Large Language Models](https://huggingface.co/docs/diffusers/en/training/lora) as this trains a smaller number of weights.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai_hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
